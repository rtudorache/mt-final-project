{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T16:47:33.753838Z",
     "start_time": "2025-12-19T16:47:28.922834Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import evaluate"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T20:10:30.416122Z",
     "start_time": "2025-12-19T20:10:30.402068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"google/mt5-base\"     # use mt5-small if VRAM is limited\n",
    "MT5_SMALL_MODEL_NAME = \"google/mt5-small\"\n",
    "TASK_PREFIX = \"translate Romanian to Rromani: \"\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LR = 3e-4\n",
    "OUTPUT_DIR = \"./mt5-small-ro-rmy\"\n",
    "CSV_PATH = \"full_corpus.csv\""
   ],
   "id": "e081341823aab580",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:18:56.714987Z",
     "start_time": "2025-12-19T17:18:56.667185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Rename if needed\n",
    "df = df.rename(columns={\"Text_ro\": \"ro\", \"Text_rom\": \"rmy\"})\n",
    "\n",
    "# Drop invalid rows\n",
    "df = df.dropna(subset=[\"ro\", \"rmy\"])\n",
    "df = df[df[\"ro\"].str.strip() != \"\"]\n",
    "df = df[df[\"rmy\"].str.strip() != \"\"]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Rows after cleaning:\", len(df))\n"
   ],
   "id": "fc4814669399ddb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 10379\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:18:58.236974Z",
     "start_time": "2025-12-19T17:18:58.205525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = dataset[\"train\"]\n",
    "eval_ds = dataset[\"test\"]\n"
   ],
   "id": "ba45d633e4573369",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:51:37.019662Z",
     "start_time": "2025-12-19T16:51:35.587534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/mt5-small\",\n",
    "    use_fast=False,   # IMPORTANT for SentencePiece\n",
    "    extra_ids=0,      # CRITICAL for translation\n",
    ")\n"
   ],
   "id": "e74a34604bd5c728",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:51:38.908907Z",
     "start_time": "2025-12-19T16:51:38.893191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(tokenizer))\n",
    "\n"
   ],
   "id": "764f70689ba2a329",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:51:55.518803Z",
     "start_time": "2025-12-19T16:51:53.710021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MT5_SMALL_MODEL_NAME)\n",
    "model = model.to('cuda')\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "next(model.parameters()).device\n"
   ],
   "id": "7d46824b49c52d8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:51:57.676108Z",
     "start_time": "2025-12-19T16:51:57.644058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SRC_LANG = \"Romanian\"\n",
    "TGT_LANG = \"Rromani\"\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = [TASK_PREFIX + x for x in batch[\"ro\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        batch[\"rmy\"],\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ],
   "id": "7464e4abe4350f45",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:07.749148Z",
     "start_time": "2025-12-19T16:52:04.957867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tok = train_ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "eval_tok = eval_ds.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=eval_ds.column_names,\n",
    "    load_from_cache_file=False,\n",
    ")\n"
   ],
   "id": "1674ab5fe154a342",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9341/9341 [00:02<00:00, 3761.18 examples/s]\n",
      "Map: 100%|██████████| 1038/1038 [00:00<00:00, 4160.80 examples/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:20.785471Z",
     "start_time": "2025-12-19T16:52:18.026403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Logits → token IDs\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # Replace -100 so we can decode\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        preds,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "\n",
    "    # Normalize (IMPORTANT)\n",
    "    decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "\n",
    "    bleu_result = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[l] for l in decoded_labels],\n",
    "        tokenize=\"intl\",\n",
    "    )\n",
    "\n",
    "    chrf_result = chrf.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[l] for l in decoded_labels],\n",
    "        word_order=2,  # chrF++\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"chrf\": chrf_result[\"score\"],\n",
    "    }"
   ],
   "id": "60dd743b4aa8ce47",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:22.005091Z",
     "start_time": "2025-12-19T16:52:21.942069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"tensorboard\",\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    ")\n"
   ],
   "id": "b78f33fd43287ce3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:23.739375Z",
     "start_time": "2025-12-19T16:52:23.712735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "id": "2dbb30d6cce23d94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:25.119651Z",
     "start_time": "2025-12-19T16:52:25.105008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def inspect_batch(dataset_tok, n=3):\n",
    "    for i in range(n):\n",
    "        labels = np.array(dataset_tok[i][\"labels\"])\n",
    "        print(i, \"len:\", len(labels), \"ignored:\", (labels == -100).sum(), \"min/max:\", labels.min(), labels.max())\n",
    "\n",
    "inspect_batch(train_tok, 5)"
   ],
   "id": "5837db2d212bb2e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 len: 84 ignored: 0 min/max: 1 216916\n",
      "1 len: 39 ignored: 0 min/max: 1 213557\n",
      "2 len: 61 ignored: 0 min/max: 1 187530\n",
      "3 len: 31 ignored: 0 min/max: 1 203783\n",
      "4 len: 45 ignored: 0 min/max: 1 197791\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:26.738432Z",
     "start_time": "2025-12-19T16:52:26.698259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ],
   "id": "9eb317d51b2400e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_88192\\1315723295.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:28.583617Z",
     "start_time": "2025-12-19T16:52:28.578409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = train_tok[0]\n",
    "\n",
    "print(\"input_ids:\", sample[\"input_ids\"][:20])\n",
    "print(\"labels:\", sample[\"labels\"][:20])\n",
    "print(\"All labels -100?\", all(l == -100 for l in sample[\"labels\"]))\n"
   ],
   "id": "10aa6ce43b2e9ed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [37194, 259, 149217, 288, 531, 63562, 266, 267, 960, 1464, 1090, 273, 259, 263, 34833, 99350, 3460, 2485, 438, 447]\n",
      "labels: [1599, 259, 216916, 259, 592, 886, 13522, 340, 62973, 3507, 340, 47274, 268, 1787, 1917, 59489, 62425, 1176, 379, 35100]\n",
      "All labels -100? False\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:17:50.782653Z",
     "start_time": "2025-12-19T16:13:48.382463Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "ee1dc18c5fe1ec5a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1119' max='11680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1119/11680 04:01 < 38:01, 4.63 it/s, Epoch 0.96/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\trainer.py:2325\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2323\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2326\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\trainer.py:2755\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2753\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2754\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[1;32m-> 2755\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallback_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_step_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2756\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_log_save_evaluate(\n\u001B[0;32m   2757\u001B[0m         tr_loss,\n\u001B[0;32m   2758\u001B[0m         grad_norm,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2764\u001B[0m         learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate,\n\u001B[0;32m   2765\u001B[0m     )\n\u001B[0;32m   2766\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\trainer_callback.py:534\u001B[0m, in \u001B[0;36mCallbackHandler.on_step_end\u001B[1;34m(self, args, state, control)\u001B[0m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mon_step_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001B[1;32m--> 534\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_event\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_step_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\trainer_callback.py:556\u001B[0m, in \u001B[0;36mCallbackHandler.call_event\u001B[1;34m(self, event, args, state, control, **kwargs)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall_event\u001B[39m(\u001B[38;5;28mself\u001B[39m, event, args, state, control, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m--> 556\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, event)(\n\u001B[0;32m    557\u001B[0m             args,\n\u001B[0;32m    558\u001B[0m             state,\n\u001B[0;32m    559\u001B[0m             control,\n\u001B[0;32m    560\u001B[0m             model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[0;32m    561\u001B[0m             processing_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessing_class,\n\u001B[0;32m    562\u001B[0m             optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer,\n\u001B[0;32m    563\u001B[0m             lr_scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler,\n\u001B[0;32m    564\u001B[0m             train_dataloader\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataloader,\n\u001B[0;32m    565\u001B[0m             eval_dataloader\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_dataloader,\n\u001B[0;32m    566\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    567\u001B[0m         )\n\u001B[0;32m    568\u001B[0m         \u001B[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001B[39;00m\n\u001B[0;32m    569\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\utils\\notebook.py:310\u001B[0m, in \u001B[0;36mNotebookProgressCallback.on_step_end\u001B[1;34m(self, args, state, control, **kwargs)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mon_step_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, args, state, control, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    309\u001B[0m     epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(state\u001B[38;5;241m.\u001B[39mepoch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mint\u001B[39m(state\u001B[38;5;241m.\u001B[39mepoch) \u001B[38;5;241m==\u001B[39m state\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate\u001B[38;5;241m.\u001B[39mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_tracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglobal_step\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEpoch \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_update\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_force_next_update\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_force_next_update \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\utils\\notebook.py:166\u001B[0m, in \u001B[0;36mNotebookProgressBar.update\u001B[1;34m(self, value, force_update, comment)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicted_remaining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;241m-\u001B[39m value)\n\u001B[1;32m--> 166\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_bar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_value \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_time \u001B[38;5;241m=\u001B[39m current_time\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\utils\\notebook.py:191\u001B[0m, in \u001B[0;36mNotebookProgressBar.update_bar\u001B[1;34m(self, value, comment)\u001B[0m\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m1\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m it/s\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\utils\\notebook.py:234\u001B[0m, in \u001B[0;36mNotebookTrainingTracker.display\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m disp\u001B[38;5;241m.\u001B[39mdisplay(disp\u001B[38;5;241m.\u001B[39mHTML(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhtml_code), display_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTML\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhtml_code\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\IPython\\core\\display_functions.py:374\u001B[0m, in \u001B[0;36mDisplayHandle.update\u001B[1;34m(self, obj, **kwargs)\u001B[0m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, obj, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    365\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Update existing displays with my id\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \n\u001B[0;32m    367\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;124;03m        additional keyword arguments passed to update_display\u001B[39;00m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 374\u001B[0m     update_display(obj, display_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\IPython\\core\\display_functions.py:326\u001B[0m, in \u001B[0;36mupdate_display\u001B[1;34m(obj, display_id, **kwargs)\u001B[0m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Update an existing display by id\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;124;03m:func:`display`\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    325\u001B[0m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 326\u001B[0m display(obj, display_id\u001B[38;5;241m=\u001B[39mdisplay_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\IPython\\core\\display_functions.py:305\u001B[0m, in \u001B[0;36mdisplay\u001B[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001B[0m\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m metadata:\n\u001B[0;32m    303\u001B[0m             \u001B[38;5;66;03m# kwarg-specified metadata gets precedence\u001B[39;00m\n\u001B[0;32m    304\u001B[0m             _merge(md_dict, metadata)\n\u001B[1;32m--> 305\u001B[0m         publish_display_data(data\u001B[38;5;241m=\u001B[39mformat_dict, metadata\u001B[38;5;241m=\u001B[39mmd_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m display_id:\n\u001B[0;32m    307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DisplayHandle(display_id)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\IPython\\core\\display_functions.py:93\u001B[0m, in \u001B[0;36mpublish_display_data\u001B[1;34m(data, metadata, source, transient, **kwargs)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transient:\n\u001B[0;32m     91\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransient\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m transient\n\u001B[1;32m---> 93\u001B[0m display_pub\u001B[38;5;241m.\u001B[39mpublish(\n\u001B[0;32m     94\u001B[0m     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m     95\u001B[0m     metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m     97\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\ipykernel\\zmqshell.py:128\u001B[0m, in \u001B[0;36mZMQDisplayPublisher.publish\u001B[1;34m(self, data, metadata, transient, update)\u001B[0m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m outputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    125\u001B[0m         outputs\u001B[38;5;241m.\u001B[39msetdefault(exec_count, [])\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m    126\u001B[0m             HistoryOutput(output_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay_data\u001B[39m\u001B[38;5;124m\"\u001B[39m, bundle\u001B[38;5;241m=\u001B[39mdata)\n\u001B[0;32m    127\u001B[0m         )\n\u001B[1;32m--> 128\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flush_streams\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\ipykernel\\zmqshell.py:76\u001B[0m, in \u001B[0;36mZMQDisplayPublisher._flush_streams\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_flush_streams\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     75\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"flush IO Streams prior to display\"\"\"\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mflush()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\ipykernel\\iostream.py:609\u001B[0m, in \u001B[0;36mOutStream.flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    607\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(evt\u001B[38;5;241m.\u001B[39mset)\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[1;32m--> 609\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    610\u001B[0m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[0;32m    611\u001B[0m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[0;32m    612\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIOStream.flush timed out\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\threading.py:607\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    605\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 607\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\threading.py:324\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 324\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    326\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:52:31.659064Z",
     "start_time": "2025-12-19T16:52:31.389980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"translate Romanian to Romani: Pavel a stat în picioare în mijlocul Areopagului.\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))"
   ],
   "id": "b2c01a945815b261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0>.</s>\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:40:10.554682Z",
     "start_time": "2025-12-19T16:40:10.269915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def debug_one_example(dataset_tok, idx=0, max_new_tokens=128, num_beams=4):\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "\n",
    "    ex = dataset_tok[idx]\n",
    "    input_ids = torch.tensor([ex[\"input_ids\"]], device=model.device)\n",
    "    attention_mask = torch.tensor([ex[\"attention_mask\"]], device=model.device)\n",
    "    labels = torch.tensor([ex[\"labels\"]], device=model.device)\n",
    "\n",
    "    # 1) Forward pass loss\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "\n",
    "    # 2) Generate prediction\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # 3) Decode nicely\n",
    "    labels_np = labels.detach().cpu().numpy()\n",
    "    labels_np = np.where(labels_np != -100, labels_np, tokenizer.pad_token_id)\n",
    "\n",
    "    src = tokenizer.decode(input_ids[0].detach().cpu().tolist(), skip_special_tokens=True)\n",
    "    ref = tokenizer.decode(labels_np[0].tolist(), skip_special_tokens=True)\n",
    "    pred = tokenizer.decode(gen_ids[0].detach().cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    print(\"DEVICE:\", model.device)\n",
    "    print(\"LOSS:\", float(loss) if loss is not None else loss)\n",
    "    print(\"\\n[SRC]\\n\", src)\n",
    "    print(\"\\n[REF]\\n\", ref)\n",
    "    print(\"\\n[PRED]\\n\", pred)\n",
    "\n",
    "# Try a few examples from eval:\n",
    "debug_one_example(eval_tok, idx=0)\n"
   ],
   "id": "2fad5370fe731f2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "LOSS: 37.58132553100586\n",
      "\n",
      "[SRC]\n",
      " translate Romanian to Rromani: Pavel a stat în picioare în mijlocul Areopagului şi a zis: „Bărbaţi atenieni! În toate privinţele vă găsesc foarte religioşi.\n",
      "\n",
      "[REF]\n",
      " Atunći o Pavel uśtilo ande punrenθe maśkar ol manuśa d‐and‐o Areopago haj phendǎs: — Manuśalen atenienǎ! Me dikhav ke tume den but pakǐv tumare develen.\n",
      "\n",
      "[PRED]\n",
      " <extra_id_0>.\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer.save_model(\"./mt5-ro-rmy-final\")\n",
    "tokenizer.save_pretrained(\"./mt5-ro-rmy-final\")"
   ],
   "id": "9c722c76320ee79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T16:36:27.519058Z",
     "start_time": "2025-12-19T16:36:27.499660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = train_tok[0]\n",
    "\n",
    "print(\"Decoded input:\")\n",
    "print(tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False))\n",
    "\n",
    "print(\"\\nDecoded labels:\")\n",
    "print(tokenizer.decode(\n",
    "    [x for x in sample[\"labels\"] if x != -100],\n",
    "    skip_special_tokens=False\n",
    "))\n"
   ],
   "id": "fa0a1499a696be87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input:\n",
      "translate Romanian to Rromani: ca să fiu slujitorul lui Isus Hristos între Neamuri. Eu îmi împlinesc cu scumpătate slujba Evangheliei lui Dumnezeu, pentruca Neamurile să -I fie o jertfă bine primită, sfinţită de Duhul Sfînt.</s>\n",
      "\n",
      "Decoded labels:\n",
      "Te avav ek pasturi le Kristosko le narodoske kai Nai Zhiduvuria, ai kadia kerav iek buchi swunto kai angerav e lashi viasta le Devleski, saxke kodola manush kai Nai Zhiduvuria aven lashe le Devleske, ai sai aven leske katar o Swunto Duxo.</s>\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:36:21.863795Z",
     "start_time": "2025-12-19T17:30:09.761359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, NllbTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 1. Model and Tokenizer\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = NllbTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set the source and target language codes\n",
    "# Romanian: ron_Latn | Rromani (Vlax/Generic): rom_Latn\n",
    "tokenizer.src_lang = \"ron_Latn\"\n",
    "tokenizer.tgt_lang = \"rom_Latn\"\n",
    "\n",
    "def preprocess_nllb(examples):\n",
    "    inputs = examples[\"ro\"]\n",
    "    targets = examples[\"rmy\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # Target tokenization\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_ds.map(preprocess_nllb, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_dataset = eval_ds.map(preprocess_nllb, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "# 2. Apply LoRA for Efficiency\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # NLLB-specific attention modules\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3. Training Arguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./nllb-ro-rom-v1\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # NLLB is more stable in FP16 than mT5\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset, # Map your preprocessed data here\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 2. Run evaluation on the UNTRAINED model\n",
    "print(\"--- Initializing Baseline Evaluation ---\")\n",
    "baseline_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"BLEU: {baseline_metrics['eval_bleu']:.2f}\")\n",
    "print(f\"chrF: {baseline_metrics['eval_chrf']:.2f}\")\n",
    "\n",
    "#trainer.train()"
   ],
   "id": "6b00df52aa2fbf4a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/9341 [00:00<?, ? examples/s]C:\\Users\\rober\\anaconda3\\envs\\model_fine-tuning-v2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 9341/9341 [00:02<00:00, 3248.02 examples/s]\n",
      "Map: 100%|██████████| 1038/1038 [00:00<00:00, 3482.32 examples/s]\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_88192\\3382651945.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Baseline Evaluation ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 05:19]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Results:\n",
      "BLEU: 0.14\n",
      "chrF: 15.20\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T19:37:19.480769Z",
     "start_time": "2025-12-19T17:39:40.640139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./nllb-rom-ron-results\",\n",
    "    # Evaluation Strategy\n",
    "    eval_strategy=\"epoch\",      # Run eval after each full pass\n",
    "    save_strategy=\"epoch\",            # Save a checkpoint so you don't lose progress\n",
    "    logging_steps=50,                 # Log training loss every 50 steps\n",
    "\n",
    "    # Hyperparameters for 8k pairs\n",
    "    learning_rate=2e-4,               # Stable for LoRA\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,    # Effective batch size 64\n",
    "    num_train_epochs=5,               # 5 passes is usually the sweet spot\n",
    "\n",
    "    # Hardware/Speed\n",
    "    fp16=True,                        # Keep enabled if your GPU supports it\n",
    "    predict_with_generate=True,       # CRITICAL for BLEU/chrF calculation\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset, # Map your preprocessed data here\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start the process\n",
    "trainer.train()"
   ],
   "id": "a14e037d20cf022c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_88192\\1169847896.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/730 1:57:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.927400</td>\n",
       "      <td>5.320968</td>\n",
       "      <td>0.298713</td>\n",
       "      <td>10.078488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.266700</td>\n",
       "      <td>4.858421</td>\n",
       "      <td>0.506031</td>\n",
       "      <td>11.107254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.958500</td>\n",
       "      <td>4.605099</td>\n",
       "      <td>0.660644</td>\n",
       "      <td>12.678128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.775200</td>\n",
       "      <td>4.480512</td>\n",
       "      <td>0.750095</td>\n",
       "      <td>13.090813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.719300</td>\n",
       "      <td>4.440754</td>\n",
       "      <td>0.851127</td>\n",
       "      <td>13.665960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=730, training_loss=5.163723065101937, metrics={'train_runtime': 7058.526, 'train_samples_per_second': 6.617, 'train_steps_per_second': 0.103, 'total_flos': 6879264787636224.0, 'train_loss': 5.163723065101937, 'epoch': 5.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T20:12:39.555808Z",
     "start_time": "2025-12-19T20:12:39.179540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"./nllb-rom-ron-results\")\n",
    "tokenizer.save_pretrained(\"./nllb-rom-ron-results\")\n",
    "\n",
    "print(\"Saved to:\", \"nllb-rom-ron-results\")"
   ],
   "id": "18e724e07ab23ee6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: nllb-rom-ron-results\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
