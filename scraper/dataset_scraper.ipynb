{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T11:30:03.953709Z",
     "start_time": "2025-12-19T11:30:01.501739Z"
    }
   },
   "source": [
    "import csv\n",
    "!pip install lxml\n",
    "!pip install datasets"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rober\\anaconda3\\envs\\mt-final-project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T10:50:13.655742Z",
     "start_time": "2025-12-19T10:50:13.653334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RROMANI_BIBLE_BASE_URL = \"https://biblia-rromani.ro\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "BIBLE_BY_MARCU_CONFIG = {\n",
    "    'n_pages': 16,\n",
    "    'url': f\"{RROMANI_BIBLE_BASE_URL}/marku/41-MRK-\"\n",
    "}\n",
    "\n",
    "BIBLE_BY_JOHN_CONFIG = {\n",
    "    'n_chapters': 21,\n",
    "    'url': f\"{RROMANI_BIBLE_BASE_URL}/joan/43-JHN-\"\n",
    "}"
   ],
   "id": "35c125e69456abcf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T10:50:15.530328Z",
     "start_time": "2025-12-19T10:50:15.527519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_page_content(page_url):\n",
    "    resp = requests.get(page_url, headers=HEADERS)\n",
    "    resp.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    return soup.find(\"div\", {\"id\": \"content\"})\n",
    "\n",
    "def clean_text(element):\n",
    "    # Remove footnotes\n",
    "    for fn in element.select(\".footnote\"):\n",
    "        fn.decompose()\n",
    "    return element.get_text(\" \", strip=True)\n"
   ],
   "id": "7111efb1c3d9b514",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T08:55:28.545755Z",
     "start_time": "2025-12-19T08:55:28.540119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extract_chapter(book, chapter, url):\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "    resp.encoding = \"utf-8\"\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(\"div\", id=\"content\")\n",
    "\n",
    "    anchors = content.find_all(\"a\", id=re.compile(r\"^v\\d+$\"))\n",
    "    rows = []\n",
    "\n",
    "    for i, a in enumerate(anchors):\n",
    "        verse_num = int(a[\"id\"][1:])\n",
    "        end = anchors[i + 1] if i + 1 < len(anchors) else None\n",
    "\n",
    "        parts = []\n",
    "        node = a.next_element\n",
    "\n",
    "        while node and node != end:\n",
    "            if getattr(node, \"name\", None) == \"div\" and \"txs\" in (node.get(\"class\") or []):\n",
    "                # remove footnotes\n",
    "                for fn in node.select(\".footnote\"):\n",
    "                    fn.decompose()\n",
    "\n",
    "                text = node.get_text(\" \", strip=True)\n",
    "                text = re.sub(r\"^\\d+\\s*\", \"\", text).strip()\n",
    "\n",
    "                if text:\n",
    "                    parts.append(text)\n",
    "\n",
    "            node = node.next_element\n",
    "\n",
    "        verse_text = \" \".join(parts)\n",
    "        verse_text = re.sub(r\"\\s+\", \" \", verse_text).strip()\n",
    "\n",
    "        rows.append([\n",
    "            book,\n",
    "            chapter,\n",
    "            verse_num,\n",
    "            verse_text\n",
    "        ])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n"
   ],
   "id": "ca89e8ab03211cd4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rromani John Bible Scraper",
   "id": "4ef8f7beed3d8df2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:05:58.833054Z",
     "start_time": "2025-12-18T16:05:58.829501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "JOHN_OUTPUT_CSV = \"john_all_chapters.csv\"\n",
    "JOHN_BASE_URL = \"https://biblia-rromani.ro/joan/43-JHN-{chapter:03d}.html\""
   ],
   "id": "eece09db1984e24",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T16:05:59.506147Z",
     "start_time": "2025-12-18T16:05:59.501565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 22):\n",
    "        url = JOHN_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing John chapter: {chapter}\")\n",
    "        rows = extract_chapter(\"John\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(JOHN_OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)\n"
   ],
   "id": "da99e0eafa0f358b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:26:20.629701Z",
     "start_time": "2025-12-17T09:26:19.631193Z"
    }
   },
   "cell_type": "code",
   "source": "run_pipeline()",
   "id": "ac77d66d9801efd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing John chapter: 1\n",
      "Processing John chapter: 2\n",
      "Processing John chapter: 3\n",
      "Processing John chapter: 4\n",
      "Processing John chapter: 5\n",
      "Processing John chapter: 6\n",
      "Processing John chapter: 7\n",
      "Processing John chapter: 8\n",
      "Processing John chapter: 9\n",
      "Processing John chapter: 10\n",
      "Processing John chapter: 11\n",
      "Processing John chapter: 12\n",
      "Processing John chapter: 13\n",
      "Processing John chapter: 14\n",
      "Processing John chapter: 15\n",
      "Processing John chapter: 16\n",
      "Processing John chapter: 17\n",
      "Processing John chapter: 18\n",
      "Processing John chapter: 19\n",
      "Processing John chapter: 20\n",
      "Processing John chapter: 21\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rromani Marcu Bible Scraper",
   "id": "1003e80c97c3703f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:29:03.089244Z",
     "start_time": "2025-12-17T09:29:03.085013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MARK_OUTPUT_CSV = \"mark_all_chapters.csv\"\n",
    "MARK_BASE_URL = \"https://biblia-rromani.ro/marku/41-MRK-{chapter:03d}.html\""
   ],
   "id": "6d84a0e770dfbb0d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:29:55.507818Z",
     "start_time": "2025-12-17T09:29:55.503429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 17):\n",
    "        url = MARK_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing John chapter: {chapter}\")\n",
    "        rows = extract_chapter(\"Mark\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(MARK_OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "7041b4a51d5235c0",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:35:27.945415Z",
     "start_time": "2025-12-17T09:35:27.554535Z"
    }
   },
   "cell_type": "code",
   "source": "run_pipeline()",
   "id": "49da1e230d05555e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing John chapter: 1\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://biblia-rromani.ro/apostola/01-GEN-001.html",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 6\u001B[39m, in \u001B[36mrun_pipeline\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      4\u001B[39m     url = GENEZA_BASE_URL.format(chapter=chapter)\n\u001B[32m      5\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mProcessing John chapter: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mchapter\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     rows = \u001B[43mextract_chapter\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mActs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchapter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     all_rows.extend(rows)\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(GENEZA_OUTPUT_CSV, \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m, newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mextract_chapter\u001B[39m\u001B[34m(book, chapter, url)\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mextract_chapter\u001B[39m(book, chapter, url):\n\u001B[32m      6\u001B[39m     resp = requests.get(url, headers={\u001B[33m\"\u001B[39m\u001B[33mUser-Agent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mMozilla/5.0\u001B[39m\u001B[33m\"\u001B[39m})\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     \u001B[43mresp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m     resp.encoding = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     10\u001B[39m     soup = BeautifulSoup(resp.text, \u001B[33m\"\u001B[39m\u001B[33mhtml.parser\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1019\u001B[39m     http_error_msg = (\n\u001B[32m   1020\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Server Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreason\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for url: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1021\u001B[39m     )\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 404 Client Error: Not Found for url: https://biblia-rromani.ro/apostola/01-GEN-001.html"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rromani Acts Bible Scraper",
   "id": "966e8b58f0eb1229"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:11:08.620899Z",
     "start_time": "2025-12-18T13:11:08.615893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ACTS_OUTPUT_CSV = \"acts_all_chapters.csv\"\n",
    "ACTS_BASE_URL = \"https://biblia-rromani.ro/apostola/44-ACT-{chapter:03d}.html\""
   ],
   "id": "d33c9d45d7530f0f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:33:29.993559Z",
     "start_time": "2025-12-17T09:33:29.989007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 29):\n",
    "        url = ACTS_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing John chapter: {chapter}\")\n",
    "        rows = extract_chapter(\"Acts\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(ACTS_OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "f613d83b6bfec314",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T09:33:39.334104Z",
     "start_time": "2025-12-17T09:33:36.085722Z"
    }
   },
   "cell_type": "code",
   "source": "run_pipeline()",
   "id": "20a26ac88f02d4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing John chapter: 1\n",
      "Processing John chapter: 2\n",
      "Processing John chapter: 3\n",
      "Processing John chapter: 4\n",
      "Processing John chapter: 5\n",
      "Processing John chapter: 6\n",
      "Processing John chapter: 7\n",
      "Processing John chapter: 8\n",
      "Processing John chapter: 9\n",
      "Processing John chapter: 10\n",
      "Processing John chapter: 11\n",
      "Processing John chapter: 12\n",
      "Processing John chapter: 13\n",
      "Processing John chapter: 14\n",
      "Processing John chapter: 15\n",
      "Processing John chapter: 16\n",
      "Processing John chapter: 17\n",
      "Processing John chapter: 18\n",
      "Processing John chapter: 19\n",
      "Processing John chapter: 20\n",
      "Processing John chapter: 21\n",
      "Processing John chapter: 22\n",
      "Processing John chapter: 23\n",
      "Processing John chapter: 24\n",
      "Processing John chapter: 25\n",
      "Processing John chapter: 26\n",
      "Processing John chapter: 27\n",
      "Processing John chapter: 28\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rromani Geneza Bible Scraper",
   "id": "abe0a1c204eb0541"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:19:40.820830Z",
     "start_time": "2025-12-18T14:19:40.809463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GENEZA_OUTPUT_CSV = \"geneza_all_chapters.csv\"\n",
    "GENEZA_BASE_URL = \"https://biblia-rromani.ro/geneza/01-GEN-{chapter:03d}.html\""
   ],
   "id": "ba3caa7570109d8f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:19:49.647137Z",
     "start_time": "2025-12-18T14:19:49.643085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 5):\n",
    "        url = GENEZA_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing Geneza chapter: {chapter}\")\n",
    "        rows = extract_chapter(\"Geneza\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(GENEZA_OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "3559de73c63c3e49",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:19:51.162976Z",
     "start_time": "2025-12-18T14:19:50.963326Z"
    }
   },
   "cell_type": "code",
   "source": "run_pipeline()",
   "id": "f31ea13f4cbe3e3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Geneza chapter: 1\n",
      "Processing Geneza chapter: 2\n",
      "Processing Geneza chapter: 3\n",
      "Processing Geneza chapter: 4\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scrape Romanian",
   "id": "77d8c2cf28b2cf2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:10:32.377397Z",
     "start_time": "2025-12-18T13:10:32.372807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_chapter_romanian(book, chapter, url):\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "    resp.encoding = \"utf-8\"\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(\"div\", id=\"biblia-text\")\n",
    "    vers_list = content.find(\"ul\", id=\"verslist\").find_all(\"li\", class_=\"verset\")\n",
    "\n",
    "    rows = []\n",
    "    for vers in vers_list:\n",
    "        vers_content = vers.find(\"a\", class_=\"verset-content\")\n",
    "\n",
    "        verse_num = vers_content.find(\"span\", class_=\"numar-verset\").find(\"sup\").text\n",
    "        verse_text = vers_content.find(\"span\", class_=\"continut-verset-simplu\").text\n",
    "\n",
    "        rows.append([\n",
    "            book,\n",
    "            chapter,\n",
    "            verse_num,\n",
    "            verse_text\n",
    "        ])\n",
    "\n",
    "    return rows"
   ],
   "id": "7e6acbe990ca8c78",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:23:02.614788Z",
     "start_time": "2025-12-18T14:23:02.610042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROMANIAN_MARK_BASE_URL = \"https://biblia.resursecrestine.ro/marcu/{chapter}\"\n",
    "ROMANIAN_MARK_OUTPUT = \"romanian_mark_all_chapters.csv\"\n",
    "\n",
    "def run_mark_romanian_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 17):\n",
    "        url = ROMANIAN_MARK_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing Mark chapter: {chapter}\")\n",
    "\n",
    "        rows = extract_chapter_romanian(\"Mark\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(ROMANIAN_MARK_OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "e648dddaa81fc91e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:23:15.997314Z",
     "start_time": "2025-12-18T14:23:05.334575Z"
    }
   },
   "cell_type": "code",
   "source": "run_mark_romanian_pipeline()",
   "id": "54263d61f00eb38f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mark chapter: 1\n",
      "Processing Mark chapter: 2\n",
      "Processing Mark chapter: 3\n",
      "Processing Mark chapter: 4\n",
      "Processing Mark chapter: 5\n",
      "Processing Mark chapter: 6\n",
      "Processing Mark chapter: 7\n",
      "Processing Mark chapter: 8\n",
      "Processing Mark chapter: 9\n",
      "Processing Mark chapter: 10\n",
      "Processing Mark chapter: 11\n",
      "Processing Mark chapter: 12\n",
      "Processing Mark chapter: 13\n",
      "Processing Mark chapter: 14\n",
      "Processing Mark chapter: 15\n",
      "Processing Mark chapter: 16\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:14:31.697698Z",
     "start_time": "2025-12-18T13:14:31.693816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROMANIAN_JOHN_BASE_URL = \"https://biblia.resursecrestine.ro/ioan/{chapter}\"\n",
    "ROMANIAN_JOHN_OUTPUT = \"romanian_john_all_chapters.csv\"\n",
    "\n",
    "def run_john_romanian_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 22):\n",
    "        url = ROMANIAN_JOHN_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing John chapter: {chapter}\")\n",
    "\n",
    "        rows = extract_chapter_romanian(\"John\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(ROMANIAN_JOHN_OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "a35d3ac09bf70c97",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:14:51.873652Z",
     "start_time": "2025-12-18T13:14:37.731072Z"
    }
   },
   "cell_type": "code",
   "source": "run_john_romanian_pipeline()",
   "id": "fdceb4d508c02ae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing John chapter: 1\n",
      "Processing John chapter: 2\n",
      "Processing John chapter: 3\n",
      "Processing John chapter: 4\n",
      "Processing John chapter: 5\n",
      "Processing John chapter: 6\n",
      "Processing John chapter: 7\n",
      "Processing John chapter: 8\n",
      "Processing John chapter: 9\n",
      "Processing John chapter: 10\n",
      "Processing John chapter: 11\n",
      "Processing John chapter: 12\n",
      "Processing John chapter: 13\n",
      "Processing John chapter: 14\n",
      "Processing John chapter: 15\n",
      "Processing John chapter: 16\n",
      "Processing John chapter: 17\n",
      "Processing John chapter: 18\n",
      "Processing John chapter: 19\n",
      "Processing John chapter: 20\n",
      "Processing John chapter: 21\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:18:32.759437Z",
     "start_time": "2025-12-18T13:18:32.755436Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 30,
   "source": [
    "ROMANIAN_GENEZA_BASE_URL = \"https://biblia.resursecrestine.ro/geneza/{chapter}\"\n",
    "ROMANIAN_GENEZA_OUTPUT = \"romanian_geneza_all_chapters.csv\"\n",
    "\n",
    "def run_geneza_romanian_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 5):\n",
    "        url = ROMANIAN_GENEZA_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing Geneza chapter: {chapter}\")\n",
    "\n",
    "        rows = extract_chapter_romanian(\"Geneza\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(ROMANIAN_GENEZA_OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "eb951b0139185dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:18:41.441386Z",
     "start_time": "2025-12-18T13:18:39.093270Z"
    }
   },
   "cell_type": "code",
   "source": "run_geneza_romanian_pipeline()",
   "id": "fb7d764f2f2e4c0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Geneza chapter: 1\n",
      "Processing Geneza chapter: 2\n",
      "Processing Geneza chapter: 3\n",
      "Processing Geneza chapter: 4\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:21:47.256857Z",
     "start_time": "2025-12-18T13:21:47.250756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROMANIAN_ACTS_BASE_URL = \"https://biblia.resursecrestine.ro/faptele-apostolilor/{chapter}\"\n",
    "ROMANIAN_ACTS_OUTPUT = \"romanian_acts_all_chapters.csv\"\n",
    "\n",
    "def run_acts_romanian_pipeline():\n",
    "    all_rows = []\n",
    "    for chapter in range(1, 29):\n",
    "        url = ROMANIAN_ACTS_BASE_URL.format(chapter=chapter)\n",
    "        print(f\"Processing Acts chapter: {chapter}\")\n",
    "\n",
    "        rows = extract_chapter_romanian(\"Acts\", chapter, url)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(ROMANIAN_ACTS_OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([\"book\", \"chapter\", \"verse\", \"text\"])\n",
    "        writer.writerows(all_rows)"
   ],
   "id": "b2cb9a676f6b2ba2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:22:13.905894Z",
     "start_time": "2025-12-18T13:21:55.520947Z"
    }
   },
   "cell_type": "code",
   "source": "run_acts_romanian_pipeline()",
   "id": "13c591961ffff04b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Acts chapter: 1\n",
      "Processing Acts chapter: 2\n",
      "Processing Acts chapter: 3\n",
      "Processing Acts chapter: 4\n",
      "Processing Acts chapter: 5\n",
      "Processing Acts chapter: 6\n",
      "Processing Acts chapter: 7\n",
      "Processing Acts chapter: 8\n",
      "Processing Acts chapter: 9\n",
      "Processing Acts chapter: 10\n",
      "Processing Acts chapter: 11\n",
      "Processing Acts chapter: 12\n",
      "Processing Acts chapter: 13\n",
      "Processing Acts chapter: 14\n",
      "Processing Acts chapter: 15\n",
      "Processing Acts chapter: 16\n",
      "Processing Acts chapter: 17\n",
      "Processing Acts chapter: 18\n",
      "Processing Acts chapter: 19\n",
      "Processing Acts chapter: 20\n",
      "Processing Acts chapter: 21\n",
      "Processing Acts chapter: 22\n",
      "Processing Acts chapter: 23\n",
      "Processing Acts chapter: 24\n",
      "Processing Acts chapter: 25\n",
      "Processing Acts chapter: 26\n",
      "Processing Acts chapter: 27\n",
      "Processing Acts chapter: 28\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:24:02.562885Z",
     "start_time": "2025-12-18T14:24:02.554660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def align_plain_text_bibles(ro_path, rom_path, output_csv):\n",
    "    with open(ro_path, 'r', encoding='utf-8') as f:\n",
    "        ro_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    with open(rom_path, 'r', encoding='utf-8') as f:\n",
    "        rom_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # Print diagnostics\n",
    "    print(f\"Romanian lines: {len(ro_lines)}\")\n",
    "    print(f\"Rromani lines: {len(rom_lines)}\")\n",
    "\n",
    "    # Check for major misalignment\n",
    "    diff = abs(len(ro_lines) - len(rom_lines))\n",
    "    if diff > 0:\n",
    "        print(f\" Warning: Files differ by {diff} lines. Data may shift!\")\n",
    "\n",
    "    # Use the shorter length to avoid IndexErrors\n",
    "    min_len = min(len(ro_lines), len(rom_lines))\n",
    "\n",
    "    aligned_data = []\n",
    "    for i in range(min_len):\n",
    "        aligned_data.append({\n",
    "            'source_ro': ro_lines[i],\n",
    "            'target_rom': rom_lines[i]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(aligned_data)\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Created parallel dataset with {min_len} pairs.\")\n",
    "\n",
    "def align_custom_dataset(base_dir):\n",
    "    # Mapping filenames to ensure we pair the right books\n",
    "    books = ['acts', 'geneza', 'john', 'mark']\n",
    "    all_frames = []\n",
    "\n",
    "    for book in books:\n",
    "        # Construct paths based on your image structure\n",
    "        ro_path = os.path.join(base_dir, 'romanian', f'romanian_{book}_all_chapters.csv')\n",
    "        rom_path = os.path.join(base_dir, 'rromani', f'{book}_all_chapters.csv')\n",
    "\n",
    "        # Load CSVs - assuming no header based on your example\n",
    "        cols = ['Book', 'Chapter', 'Verse', 'Text']\n",
    "        df_ro = pd.read_csv(ro_path, names=cols, header=None)\n",
    "        df_rom = pd.read_csv(rom_path, names=cols, header=None)\n",
    "\n",
    "        # Create a unique key for matching: \"Acts_1_1\"\n",
    "        df_ro['ID'] = df_ro['Book'] + \"_\" + df_ro['Chapter'].astype(str) + \"_\" + df_ro['Verse'].astype(str)\n",
    "        df_rom['ID'] = df_rom['Book'] + \"_\" + df_rom['Chapter'].astype(str) + \"_\" + df_rom['Verse'].astype(str)\n",
    "\n",
    "        # Perform an 'inner' merge to keep only verses present in both\n",
    "        merged = pd.merge(\n",
    "            df_ro[['ID', 'Text']],\n",
    "            df_rom[['ID', 'Text']],\n",
    "            on='ID',\n",
    "            suffixes=('_ro', '_rom')\n",
    "        )\n",
    "\n",
    "        print(f\"Book: {book} | Matched {len(merged)} verses.\")\n",
    "        all_frames.append(merged)\n",
    "\n",
    "    # Combine all books into one master training file\n",
    "    final_df = pd.concat(all_frames, ignore_index=True)\n",
    "    final_df.to_csv('merged_parallel_corpus.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\n✅ Total Parallel Pairs: {len(final_df)}\")\n",
    "    return final_df"
   ],
   "id": "d8fcdc9df54c16d4",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:09:26.370448Z",
     "start_time": "2025-12-18T14:09:26.329034Z"
    }
   },
   "cell_type": "code",
   "source": "align_plain_text_bibles('../bible-uedin.ro-rom.ro', '../bible-uedin.ro-rom.rom', 'uedin_ro_rom.csv')",
   "id": "1541c28b65d850db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romanian lines: 7931\n",
      "Rromani lines: 7931\n",
      "✅ Created parallel dataset with 7931 pairs.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:24:29.001713Z",
     "start_time": "2025-12-18T14:24:28.965088Z"
    }
   },
   "cell_type": "code",
   "source": "custom_dataset = align_custom_dataset('../data')",
   "id": "b489bbc27547acd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: acts | Matched 1007 verses.\n",
      "Book: geneza | Matched 106 verses.\n",
      "Book: john | Matched 879 verses.\n",
      "Book: mark | Matched 678 verses.\n",
      "\n",
      "✅ Total Parallel Pairs: 2670\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Merging and Preprocessing\n",
    "\n",
    "- Deduplication\n",
    "- Normalizaiton\n",
    "- Length Filtering"
   ],
   "id": "24db34934948f15e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T11:29:41.411649Z",
     "start_time": "2025-12-19T11:29:41.409031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "CSV_PATH = \"full_corpus.csv\"\n",
    "OUTPUT_PATH = \"./nllb_ro_rromani_lora\"\n"
   ],
   "id": "2848e802990cba75",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T11:29:50.349192Z",
     "start_time": "2025-12-19T11:29:50.346786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize(text):\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ],
   "id": "13ccb6b468b6cf58",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T11:31:16.697129Z",
     "start_time": "2025-12-19T11:31:16.416431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "custom_df = pd.read_csv('merged_parallel_corpus.csv')\n",
    "uedin_df = pd.read_csv('uedin_ro_rom.csv')\n",
    "\n",
    "custom_df[\"source\"] = \"custom\"\n",
    "uedin_df[\"source\"] = \"uedin\"\n",
    "\n",
    "custom_df = custom_df.rename(columns={'Text_ro': 'ro', 'Text_rom': 'rmy'})\n",
    "uedin_df = uedin_df.rename(columns={'source_ro': 'ro', 'target_rom': 'rmy'})\n",
    "\n",
    "df = pd.concat([custom_df, uedin_df], ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['ro', 'rmy'])\n",
    "\n",
    "conflicts = (\n",
    "    df.groupby(\"ro\")\n",
    "    .filter(lambda x: len(x) > 1)\n",
    ")\n",
    "\n",
    "print(\"RO sentences with multiple translations:\", conflicts[\"ro\"].nunique())\n",
    "\n",
    "df[\"ro\"] = df[\"ro\"].apply(normalize)\n",
    "df[\"rmy\"] = df[\"rmy\"].apply(normalize)\n",
    "\n",
    "df[\"len_ro\"] = df[\"ro\"].str.split().str.len()\n",
    "df[\"len_rmy\"] = df[\"rmy\"].str.split().str.len()\n",
    "\n",
    "df[\"ratio\"] = df[\"len_rmy\"] / df[\"len_ro\"]\n",
    "\n",
    "df = df[(df[\"ratio\"] > 0.5) & (df[\"ratio\"] < 2.0)]\n",
    "\n",
    "print(\"Final rows:\", len(df))\n",
    "print(df.sample(5))\n",
    "\n",
    "df.to_csv(CSV_PATH, index=False, encoding='utf-8')\n",
    "# train_df, eval_df = train_test_split(df_all, test_size=0.1, random_state=42)"
   ],
   "id": "87d551011fe5b415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RO sentences with multiple translations: 107\n",
      "Final rows: 10379\n",
      "            ID                                                 ro  \\\n",
      "8626       NaN  Dar Scriptura a închis totul supt păcat, pentr...   \n",
      "7344       NaN  ,Cine eşti, Doamne?` am răspuns eu. Şi Domnul ...   \n",
      "8788       NaN  ci, credincioşi adevărului, în dragoste, să cr...   \n",
      "2073  Mark_3_9  Isus a poruncit ucenicilor să-I ţină la îndemâ...   \n",
      "3857       NaN  dar n'au rădăcină în ei, ci ţin pînă la o vrem...   \n",
      "\n",
      "                                                    rmy  source  len_ro  \\\n",
      "8626  Numa E Vorba le Devleski mothol ke sa e lumia ...   uedin      20   \n",
      "7344  Ai me phendem, \"Kon san tu Devla,\" ai o Del ph...   uedin      17   \n",
      "8788  Numa phenas o chachimos la dragostiasa, sagda ...   uedin      19   \n",
      "2073  O Isus porunćisardǎs le ućenićenqe te anen Les...  custom      17   \n",
      "3857  Numa chi gelo divano dur ande lengo ilo ai nas...   uedin      27   \n",
      "\n",
      "      len_rmy     ratio  \n",
      "8626       36  1.800000  \n",
      "7344       11  0.647059  \n",
      "8788       33  1.736842  \n",
      "2073       19  1.117647  \n",
      "3857       42  1.555556  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T11:31:21.149314Z",
     "start_time": "2025-12-19T11:31:21.146613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.columns)\n",
    "print(\"Rows:\", len(df))"
   ],
   "id": "3b2c69c6a3422c29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'ro', 'rmy', 'source', 'len_ro', 'len_rmy', 'ratio'], dtype='object')\n",
      "Rows: 10379\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./nllb_ro_rromani_lora\"\n",
    "MAX_LEN = 256\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[\"ro\"] = df[\"ro\"].astype(str)\n",
    "df[\"rmy\"] = df[\"rmy\"].astype(str)\n",
    "\n",
    "df = df[\n",
    "    df[\"ro\"].notna() &\n",
    "    df[\"rmy\"].notna() &\n",
    "    (df[\"ro\"].str.strip() != \"\") &\n",
    "    (df[\"rmy\"].str.strip() != \"\")\n",
    "]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Rows after cleaning:\", len(df))\n",
    "\n",
    "ds = Dataset.from_pandas(df[[\"ro\", \"rmy\"]])\n",
    "ds = ds.train_test_split(test_size=0.08, seed=42)  # 92/8 split\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "SRC_LANG = \"ron_Latn\"\n",
    "TGT_LANG = \"rom_Latn\"\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(TGT_LANG)\n",
    "\n",
    "def preprocess(batch):\n",
    "    ro_list = batch.get(\"ro\")\n",
    "    rmy_list = batch.get(\"rmy\")\n",
    "\n",
    "    # Arrow may give empty batches\n",
    "    if ro_list is None or rmy_list is None:\n",
    "        return {}\n",
    "\n",
    "    cleaned_ro = []\n",
    "    cleaned_rmy = []\n",
    "\n",
    "    for ro, rmy in zip(ro_list, rmy_list):\n",
    "        if ro and rmy:\n",
    "            ro = str(ro).strip()\n",
    "            rmy = str(rmy).strip()\n",
    "            if ro != \"\" and rmy != \"\":\n",
    "                cleaned_ro.append(ro)\n",
    "                cleaned_rmy.append(rmy)\n",
    "\n",
    "    # IMPORTANT: skip empty batches\n",
    "    if len(cleaned_ro) == 0:\n",
    "        return {}\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        cleaned_ro,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        cleaned_rmy,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_tok = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)"
   ],
   "id": "bf4a5b18780f8f2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---- 5) Load model in 4-bit and attach LoRA ----\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA config for seq2seq (target attention projections)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure correct target language during generation\n",
    "model.config.forced_bos_token_id = forced_bos_token_id\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ---- 6) Metrics ----\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Replace -100 with pad token for decoding\n",
    "    labels = [[(t if t != -100 else tokenizer.pad_token_id) for t in seq] for seq in labels]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels]).score\n",
    "    chrf = sacrebleu.corpus_chrf(decoded_preds, [decoded_labels]).score\n",
    "    return {\"bleu\": bleu, \"chrf\": chrf}\n",
    "\n",
    "# ---- 7) Training args ----\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,   # effective batch 32\n",
    "    learning_rate=2e-4,              # LoRA can tolerate higher LR\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_LEN,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n"
   ],
   "id": "11165b376e039e91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import sacrebleu\n",
    "\n",
    "OUTPUT_DIR = \"./nllb_ro_rromani_lora\"\n",
    "\n",
    "# ---- 1) Load data ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.copy()\n",
    "\n",
    "# Basic cleaning (minimal; avoid over-normalization)\n",
    "def clean(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "df[\"ro\"] = df[\"ro\"].astype(str)\n",
    "df[\"rmy\"] = df[\"rmy\"].astype(str)\n",
    "\n",
    "# drop invalid rows\n",
    "df = df[\n",
    "    df[\"ro\"].notna() &\n",
    "    df[\"rmy\"].notna() &\n",
    "    (df[\"ro\"].str.strip() != \"\") &\n",
    "    (df[\"rmy\"].str.strip() != \"\")\n",
    "]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"ro\"] = df[\"ro\"].map(clean)\n",
    "df[\"rmy\"] = df[\"rmy\"].map(clean)\n",
    "\n",
    "df = df[(df[\"ro\"] != \"\") & (df[\"rmy\"] != \"\")]\n",
    "df = df.drop_duplicates(subset=[\"ro\", \"rmy\"]).reset_index(drop=True)\n",
    "\n",
    "assert df[\"ro\"].apply(lambda x: isinstance(x, str)).all()\n",
    "assert df[\"rmy\"].apply(lambda x: isinstance(x, str)).all()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "SRC_LANG = \"ron_Latn\"\n",
    "TGT_LANG = \"rom_Latn\"\n",
    "\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(TGT_LANG)\n",
    "\n",
    "# ---- 3) Train/val split ----\n",
    "ds = Dataset.from_pandas(df[[\"ro\", \"rmy\"]])\n",
    "ds = ds.train_test_split(test_size=0.08, seed=42)  # 92/8 split\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "# ---- 4) Preprocess ----\n",
    "MAX_LEN = 256\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = batch[\"ro\"]\n",
    "    targets = batch[\"rmy\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_tok = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "# ---- 5) Load model in 4-bit and attach LoRA ----\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA config for seq2seq (target attention projections)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure correct target language during generation\n",
    "model.config.forced_bos_token_id = forced_bos_token_id\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# ---- 6) Metrics ----\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Replace -100 with pad token for decoding\n",
    "    labels = [[(t if t != -100 else tokenizer.pad_token_id) for t in seq] for seq in labels]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels]).score\n",
    "    chrf = sacrebleu.corpus_chrf(decoded_preds, [decoded_labels]).score\n",
    "    return {\"bleu\": bleu, \"chrf\": chrf}\n",
    "\n",
    "# ---- 7) Training args ----\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,   # effective batch 32\n",
    "    learning_rate=2e-4,              # LoRA can tolerate higher LR\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_LEN,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n"
   ],
   "id": "e631c6b18ee0762c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mt-final-project]",
   "language": "python",
   "name": "conda-env-mt-final-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
